# üß† memories.dev

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: Apache-2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Tests](https://github.com/Vortx-AI/memories-dev/actions/workflows/tests.yml/badge.svg)](https://github.com/Vortx-AI/memories-dev/actions/workflows/tests.yml)
[![Documentation Status](https://readthedocs.org/projects/memoriesdev/badge/?version=latest)](https://memoriesdev.readthedocs.io/en/latest/?badge=latest)

> Building the future of collective AGI memory - v1.0.0

## üåü Overview

memories.dev is a groundbreaking framework for building and managing collective AGI memory systems. It provides a robust architecture for memory formation, retrieval, and synthesis across multiple modalities, enabling AI models to maintain and utilize contextual understanding across interactions.

### üéØ Key Goals
- Enable persistent memory for AI systems
- Provide context-aware intelligence
- Support multi-modal memory integration
- Ensure scalable and efficient memory operations
- Maintain privacy and security in memory access

## üöÄ Quick Start

```python
from memories_dev import MemorySystem
from memories_dev.agents import Agent
from memories_dev.models import ModelRegistry

# Initialize the memory system
memory_system = MemorySystem(
    store_type="vector",  # Options: "vector", "graph", "hybrid"
    vector_store="milvus",  # Coming in v1.1: Support for multiple vector stores
    embedding_model="text-embedding-3-small"
)

# Create an agent with memory access
agent = Agent(
    memory_system=memory_system,
    model_name="gpt-4",  # Default model for reasoning
    capabilities=["analysis", "synthesis"]
)

# Store a memory
memory_id = memory_system.store(
    content="The city's air quality improved by 15% after implementing new policies.",
    metadata={
        "location": {"lat": 37.7749, "lon": -122.4194},
        "timestamp": "2024-03-15T10:30:00Z",
        "source": "environmental_sensor"
    }
)

# Query memories with context
relevant_memories = memory_system.query(
    query="What were the environmental changes in San Francisco?",
    location_radius_km=10,
    time_range=("2024-01-01", "2024-03-15")
)

# Agent reasoning with memory context
analysis = agent.analyze(
    query="Evaluate the impact of environmental policies",
    context_memories=relevant_memories
)
```

## üèóÔ∏è Installation

### Current Release (v1.0.0)
```bash
# Basic installation
pip install git+https://github.com/Vortx-AI/memories-dev.git

# Development installation
git clone https://github.com/Vortx-AI/memories-dev.git
cd memories-dev
pip install -e ".[dev]"
```

### Coming in v1.1.0
```bash
# Installation with all features
pip install memories-dev[all]

# GPU-optimized installation
pip install memories-dev[gpu]
```

## üîß System Requirements

### Minimum (Development)
- Python 3.9+
- 16GB RAM
- 4+ CPU cores
- 20GB storage
- Docker & Docker Compose (for local development)

### Production (Recommended)
- 32GB+ RAM
- 8+ CPU cores
- NVIDIA GPU with 8GB+ VRAM
- 100GB+ SSD storage
- Kubernetes cluster for distributed deployment

## üìä Monitoring & Observability

### Available Now (v1.0.0)
- Basic logging system with structured output
- Memory operation metrics
- Performance tracking for core operations
- Health check endpoints

### Coming in v1.1.0
- üìà Grafana dashboards for memory metrics
- üîç Prometheus integration
- üîÑ Real-time memory operation monitoring
- üìä Advanced performance analytics
- üö® Automated alerting system

## üß™ Development Features

### Available Now
- Memory store implementations
- Basic agent system
- Core memory operations
- Unit test framework
- Development environment setup

### Coming Soon (v1.1.0)
- Enhanced memory compression
- Advanced caching system
- Distributed memory operations
- Memory garbage collection
- Advanced security features

## üìÅ Project Structure

```
memories-dev/
‚îú‚îÄ‚îÄ üöÄ src/                    # Source code
‚îÇ   ‚îú‚îÄ‚îÄ agents/               # Agent System
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core/            # Core agent functionality
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py      # Base agent classes
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ registry.py  # Agent registry
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.py    # Agent configurations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ memory/          # Memory integration
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ context.py   # Context management
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ retrieval.py # Memory retrieval
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cache.py     # Memory caching
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ specialized/     # Specialized agents
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ analysis.py  # Analysis agents
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ synthesis.py # Synthesis agents
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ learning.py  # Learning agents
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ data_acquisition/    # Data Collection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ satellite/       # Satellite data
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sentinel/   # Sentinel-1/2 handlers
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ landsat/    # Landsat handlers
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ modis/      # MODIS data handlers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sensors/        # Sensor networks
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ climate/    # Climate sensors
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ urban/      # Urban sensors
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ iot/        # IoT device handlers
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ streams/        # Real-time streams
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ ingest.py   # Stream ingestion
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ process.py  # Stream processing
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ buffer.py   # Stream buffering
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ memories/           # Memory System
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ store/         # Storage backend
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vector.py  # Vector store
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ graph.py   # Graph store
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hybrid.py  # Hybrid storage
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ formation/     # Memory creation
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ create.py  # Memory formation
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ update.py  # Memory updates
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ merge.py   # Memory merging
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ query/         # Query system
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ spatial.py # Spatial queries
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ temporal.py# Temporal queries
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ semantic.py# Semantic queries
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ optimization/  # Memory optimization
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ compress.py# Compression
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ prune.py   # Memory pruning
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/            # AI Models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedding/     # Embedding models
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ text.py   # Text embeddings
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vision.py # Vision embeddings
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ audio.py  # Audio embeddings
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ reasoning/    # Reasoning models
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm.py   # Language models
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chain.py # Reasoning chains
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fusion/      # Multi-modal fusion
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ early.py # Early fusion
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ late.py  # Late fusion
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ synthesis/        # Memory Synthesis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fusion/      # Data fusion
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ spatial.py # Spatial fusion
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ temporal.py# Temporal fusion
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ generation/  # Synthetic data
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ augment.py # Data augmentation
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ create.py  # Synthetic creation
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/           # Utilities
‚îÇ       ‚îú‚îÄ‚îÄ config/      # Configuration
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ settings.py # Global settings
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ env.py     # Environment vars
‚îÇ       ‚îú‚îÄ‚îÄ logging/     # Logging system
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ logger.py  # Logger setup
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ metrics.py # Metrics logging
‚îÇ       ‚îî‚îÄ‚îÄ validation/  # Data validation
‚îÇ           ‚îú‚îÄ‚îÄ schema.py  # Schema validation
‚îÇ           ‚îî‚îÄ‚îÄ types.py   # Type checking
‚îÇ
‚îú‚îÄ‚îÄ üìä monitoring/        # Monitoring & Metrics
‚îÇ   ‚îú‚îÄ‚îÄ grafana/         # Grafana dashboards
‚îÇ   ‚îú‚îÄ‚îÄ prometheus/      # Prometheus configs
‚îÇ   ‚îî‚îÄ‚îÄ alerts/          # Alert configurations
‚îÇ
‚îú‚îÄ‚îÄ üß™ tests/            # Test Suite
‚îÇ   ‚îú‚îÄ‚îÄ unit/           # Unit tests
‚îÇ   ‚îú‚îÄ‚îÄ integration/    # Integration tests
‚îÇ   ‚îî‚îÄ‚îÄ performance/    # Performance tests
‚îÇ
‚îú‚îÄ‚îÄ üìö docs/             # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ api/            # API documentation
‚îÇ   ‚îú‚îÄ‚îÄ guides/         # User guides
‚îÇ   ‚îî‚îÄ‚îÄ examples/       # Example notebooks
‚îÇ
‚îú‚îÄ‚îÄ üõ†Ô∏è scripts/          # Development scripts
‚îÇ   ‚îú‚îÄ‚îÄ setup/          # Setup scripts
‚îÇ   ‚îú‚îÄ‚îÄ deploy/         # Deployment scripts
‚îÇ   ‚îî‚îÄ‚îÄ maintenance/    # Maintenance tools
‚îÇ
‚îî‚îÄ‚îÄ üì¶ docker/           # Docker configurations
    ‚îú‚îÄ‚îÄ dev/            # Development setup
    ‚îú‚îÄ‚îÄ prod/           # Production setup
    ‚îî‚îÄ‚îÄ compose/        # Docker Compose files
```

## üß© Core Components Explained

### 1. ü§ñ Agent System
The agent system is the intelligence layer that orchestrates memory operations:

```python
from memories_dev.agents import SpecializedAgent

# Create an analysis agent with specific capabilities
agent = SpecializedAgent(
    type="analysis",
    capabilities={
        "spatial_analysis": True,
        "temporal_analysis": True,
        "pattern_recognition": True
    },
    memory_access_level="read_write"
)

# Execute complex analysis
results = agent.analyze(
    data_sources=["satellite", "sensors"],
    time_range=("2024-01", "2024-03"),
    analysis_type="trend_detection"
)
```

### 2. üì° Data Acquisition
Handles multi-modal data ingestion with built-in preprocessing:

```python
from memories_dev.data_acquisition import DataCollector

# Initialize a multi-source collector
collector = DataCollector(
    sources={
        "satellite": {
            "type": "sentinel-2",
            "bands": ["B02", "B03", "B04", "B08"]
        },
        "climate": {
            "type": "weather_station",
            "metrics": ["temperature", "humidity", "air_quality"]
        }
    },
    preprocessing_pipeline=[
        "normalize",
        "filter_outliers",
        "align_timestamps"
    ]
)

# Collect and preprocess data
data = collector.collect(
    location=(37.7749, -122.4194),
    timeframe="1d",
    resolution="10m"
)
```

### 3. üß† Memory Formation
Advanced memory creation and optimization:

```python
from memories_dev.memories import MemoryFormation

# Initialize memory formation with optimization
memory_former = MemoryFormation(
    optimization={
        "compression": "lossless",
        "deduplication": True,
        "priority_ranking": True
    },
    storage_strategy="hybrid"
)

# Form optimized memory
memory = memory_former.create(
    content=data,
    metadata={
        "source": "environmental_monitoring",
        "confidence": 0.95,
        "importance": "high"
    },
    relationships=[
        {"type": "spatial", "ref": "nearby_memories"},
        {"type": "temporal", "ref": "previous_state"}
    ]
)
```

## üéØ Features (v1.0.0)

### Core Capabilities
- üîÑ Real-time memory formation and updates
- üîç Advanced spatial and temporal querying
- ü§ù Multi-agent collaboration
- üéØ Context-aware memory retrieval
- üîó Cross-modal memory linking

### Technical Features
- ‚ö° High-performance vector storage
- üîê Secure memory management
- üìä Advanced data fusion algorithms
- üé® Multi-modal data support
- üîÑ Real-time stream processing

## üöÄ Roadmap

### Coming in v1.1.0
- Enhanced memory compression
- Improved cross-modal reasoning
- Advanced context understanding

### Future Horizons
- Distributed memory networks
- Quantum-inspired memory storage
- Advanced consciousness simulation

## üõ†Ô∏è Development

### Prerequisites
- Python 3.8+
- CUDA-compatible GPU (recommended)
- Vector store backend (Redis/Milvus/Pinecone)
- Docker & Docker Compose
- Make (optional, for using Makefile commands)

### Environment Setup

1. **Clone and Setup**
```bash
# Clone the repository
git clone https://github.com/yourusername/memories.dev.git
cd memories.dev

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: .\venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
pip install -r requirements-dev.txt  # Development dependencies
```

2. **Environment Variables**
```bash
# Copy example environment file
cp .env.example .env

# Edit .env with your configurations
nano .env
```

3. **Development Database**
```bash
# Start development services
docker-compose up -d
```

### Development Workflow

#### 1. Code Style
We use `black` for code formatting and `flake8` for linting:
```bash
# Format code
make format

# Run linting
make lint
```

#### 2. Testing
```bash
# Run all tests
make test

# Run specific test file
pytest tests/test_memories/test_store.py

# Run with coverage
make coverage
```

#### 3. Pre-commit Hooks
```bash
# Install pre-commit hooks
pre-commit install

# Run hooks manually
pre-commit run --all-files
```

### üê≥ Docker Development

```bash
# Build development image
docker build -t memories-dev -f docker/Dockerfile.dev .

# Run development container
docker run -it --gpus all -v $(pwd):/app memories-dev
```

### üìä Monitoring & Debugging

1. **Logging**
```python
# In your code
from memories.utils.logging import get_logger

logger = get_logger(__name__)
logger.info("Memory operation completed")
```

2. **Performance Monitoring**
- Access metrics dashboard: `http://localhost:8080/metrics`
- Prometheus integration available
- Grafana dashboards in `./monitoring`

### üîÑ Common Workflows

#### Adding a New Feature
1. Create feature branch: `git checkout -b feature/your-feature`
2. Implement changes
3. Add tests in `tests/`
4. Update documentation
5. Run test suite: `make test`
6. Create pull request

#### Updating Dependencies
1. Update `requirements.in` or `requirements-dev.in`
2. Compile requirements: `make requirements`
3. Test changes: `make test`
4. Commit changes

## üöÄ CI/CD Pipeline

```mermaid
graph LR
    A[Push] --> B[Lint]
    B --> C[Test]
    C --> D[Build]
    D --> E[Deploy]
```

### Automated Checks
- Code formatting (black)
- Type checking (mypy)
- Unit tests (pytest)
- Integration tests
- Security scanning
- Performance benchmarks

### Deployment Environments
- **Dev**: Automatic deployment on main branch
- **Staging**: Manual trigger from release branches
- **Production**: Tagged releases only

## üìö Documentation

### Building Docs Locally
```bash
# Install documentation dependencies
pip install -r docs/requirements.txt

# Build documentation
cd docs
make html

# Serve documentation
python -m http.server -d _build/html
```

### API Documentation
- [API Reference](docs/api.md)
- [Architecture Guide](docs/architecture.md)
- [Development Guide](docs/development.md)
- [Deployment Guide](docs/deployment.md)
- [Contributing Guide](CONTRIBUTING.md)

### Example Notebooks
Find example notebooks in `notebooks/`:
- üî∞ Quick Start
- üß† Memory Operations
- ü§ñ Agent Integration
- üìä Data Visualization

## üéì Developer Resources

### Tutorials & Guides
- [Getting Started Guide](docs/getting_started.md)
- [Memory System Architecture](docs/architecture.md)
- [Agent Development Guide](docs/agents.md)
- [Custom Memory Store Integration](docs/custom_stores.md)

### Example Use Cases
1. **Persistent Context in Conversations**
   ```python
   # Example of maintaining context across conversations
   conversation = memory_system.create_conversation()
   conversation.add_memory("User preference: Dark mode")
   conversation.add_memory("Language: Python")
   
   # Later in the conversation
   context = conversation.get_relevant_context("IDE settings")
   ```

2. **Spatial Memory Analysis**
   ```python
   # Analyzing patterns in spatial memories
   spatial_analysis = memory_system.analyze_spatial(
       location=(37.7749, -122.4194),
       radius_km=5,
       time_range=("2024-01-01", "2024-03-15"),
       analysis_type="pattern_recognition"
   )
   ```

### Best Practices
- Always use context managers for memory operations
- Implement proper error handling
- Use batch operations for multiple memories
- Regularly clean up unused memories
- Monitor memory usage and performance

## ü§ù Community & Support

- [Discord Community](https://discord.gg/memoriesdev)
- [GitHub Discussions](https://github.com/Vortx-AI/memories-dev/discussions)
- [Documentation](https://memoriesdev.readthedocs.io/)
- [Blog](https://memoriesdev.medium.com)

## ü§ù Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

### Getting Started
1. Fork the repository
2. Create your feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

### Code Review Process
1. Automated checks must pass
2. Requires one approved review
3. Must maintain test coverage
4. Documentation updates required

## üìú Version History

### v1.0.0 (Current)
- Initial stable release
- Core memory system implementation
- Basic agent framework
- Multi-modal data support

### v0.9.0 (Beta)
- Feature-complete beta release
- Performance optimizations
- API stabilization

## üìù License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- OpenAI for GPT models
- Hugging Face for transformers
- FastAPI team for the web framework
- Our amazing contributors

---

<p align="center">Built with üíú by the memories.dev team</p>

<p align="center">
<a href="https://discord.gg/memoriesdev">Discord</a> ‚Ä¢
<a href="https://twitter.com/memoriesdev">Twitter</a> ‚Ä¢
<a href="https://memoriesdev.medium.com">Blog</a>
</p>

## üîå Integration Examples

### 1. Real-time Environmental Monitoring
```python
from memories_dev import MemorySystem
from memories_dev.data_acquisition import StreamCollector
from memories_dev.synthesis import RealTimeSynthesis

# Setup real-time monitoring
stream = StreamCollector(
    sources=["air_quality", "traffic", "noise"],
    buffer_size="1h",
    sampling_rate="1m"
)

# Process and store real-time data
@stream.on_data
async def process_environmental_data(data):
    # Form memory from real-time data
    memory = await memory_system.store_streaming(
        data,
        retention_policy="24h",
        importance_threshold=0.7
    )
    
    # Trigger real-time synthesis
    insights = await RealTimeSynthesis.analyze(
        memory,
        context_window="6h"
    )
```

### 2. Multi-modal Memory Fusion
```python
from memories_dev.fusion import ModalityFusion
from memories_dev.models import MultiModalEncoder

# Initialize fusion system
fusion = ModalityFusion(
    modalities=["text", "vision", "sensor"],
    fusion_strategy="hierarchical",
    alignment="temporal"
)

# Fuse different memory types
fused_memory = fusion.combine(
    memories={
        "text": text_memory,
        "vision": image_memory,
        "sensor": sensor_data
    },
    weights={
        "text": 0.4,
        "vision": 0.4,
        "sensor": 0.2
    }
)
```
