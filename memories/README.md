# ðŸ§  memories.dev

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: Apache-2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Tests](https://github.com/Vortx-AI/memories-dev/actions/workflows/tests.yml/badge.svg)](https://github.com/Vortx-AI/memories-dev/actions/workflows/tests.yml)

> Collective AGI memory - v1.1.2 (February 16, 2024)

## ðŸŒŸ Overview

memories.dev is a groundbreaking framework for building and managing collective AGI memory systems. It provides a robust architecture for memory formation, retrieval, and synthesis across multiple modalities, enabling AI models to maintain and utilize contextual understanding across interactions.

### ðŸŽ¯ Key Goals
- Enable persistent memory for AI systems
- Provide context-aware intelligence
- Support multi-modal memory integration
- Ensure scalable and efficient memory operations
- Maintain privacy and security in memory access

## ðŸš€ Quick Start

```python
from memories_dev import MemorySystem
from memories_dev.models import ModelRegistry

# Initialize the memory system
memory_system = MemorySystem(
    store_type="vector",  # Options: "vector", "graph", "hybrid"
    vector_store="milvus",  # Coming in v1.1: Support for multiple vector stores
    embedding_model="text-embedding-3-small"
)



# Store a memory
memory_id = memory_system.store(
    content="The city's air quality improved by 15% after implementing new policies.",
    metadata={
        "location": {"lat": 37.7749, "lon": -122.4194},
        "timestamp": "2024-03-15T10:30:00Z",
        "source": "environmental_sensor"
    }
)

# Query memories with context
relevant_memories = memory_system.query(
    query="What were the environmental changes in San Francisco?",
    location_radius_km=10,
    time_range=("2024-01-01", "2024-03-15")
)


```

## ðŸ—ï¸ Installation

### Current Release (v1.0.0)
```bash
# Basic installation
pip install git+https://github.com/Vortx-AI/memories-dev.git

# Development installation
git clone https://github.com/Vortx-AI/memories-dev.git
cd memories-dev
pip install -e ".[dev]"
```

### Coming in v1.1.0
```bash
# Installation with all features
pip install memories-dev[all]

# GPU-optimized installation
pip install memories-dev[gpu]
```

## ðŸ”§ System Requirements

### Minimum (Development)
- Python 3.9+
- 16GB RAM
- 4+ CPU cores
- 20GB storage
- Docker & Docker Compose (for local development)

### Production (Recommended)
- 32GB+ RAM
- 8+ CPU cores
- NVIDIA GPU with 8GB+ VRAM
- 100GB+ SSD storage
- Kubernetes cluster for distributed deployment

## ðŸ“Š Monitoring & Observability

### Available Now (v1.0.0)
- Basic logging system with structured output
- Memory operation metrics
- Performance tracking for core operations
- Health check endpoints

### Coming in v1.1.0
- ðŸ“ˆ Grafana dashboards for memory metrics
- ðŸ” Prometheus integration
- ðŸ”„ Real-time memory operation monitoring
- ðŸ“Š Advanced performance analytics
- ðŸš¨ Automated alerting system

## ðŸ§ª Development Features

### Available Now
- Memory store implementations
- Core memory operations
- Unit test framework
- Development environment setup

### Coming Soon (v1.1.0)
- Enhanced memory compression
- Advanced caching system
- Distributed memory operations
- Memory garbage collection
- Advanced security features

## ðŸ“ Project Structure

```
memories-dev/
â”œâ”€â”€ examples/              # Example implementations
â”‚   â”œâ”€â”€ basic/            # Basic usage examples
â”‚   â”œâ”€â”€ advanced/         # Advanced implementations
â”‚   â””â”€â”€ notebooks/        # Jupyter notebooks
â”‚
â”œâ”€â”€ memories_dev/         # Main package
â”‚   â”œâ”€â”€ models/           # Models
â”‚   â”‚   â”œâ”€â”€ core/       # Core functionality
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py     # Base  classes
â”‚   â”‚   â”‚   â””â”€â”€ registry.py #  registry
â”‚   â”‚   â”œâ”€â”€ memory/     # Memory integration
â”‚   â”‚   â”‚   â”œâ”€â”€ context.py  # Context management
â”‚   â”‚   â”‚   â””â”€â”€ retrieval.py# Memory retrieval
â”‚   â”‚   â””â”€â”€ specialized/# Specialized 
â”‚   â”‚       â”œâ”€â”€ analysis.py # Analysis 
â”‚   â”‚       â””â”€â”€ synthesis.py# Synthesis 
â”‚   â”‚
â”‚   â”œâ”€â”€ data_acquisition/# Data Collection
â”‚   â”‚   â”œâ”€â”€ satellite/   # Satellite data handlers
â”‚   â”‚   â”‚   â”œâ”€â”€ sentinel/# Sentinel-1/2
â”‚   â”‚   â”‚   â””â”€â”€ landsat/ # Landsat 7/8
â”‚   â”‚   â”œâ”€â”€ sensors/    # Sensor networks
â”‚   â”‚   â”‚   â”œâ”€â”€ climate/# Climate sensors
â”‚   â”‚   â”‚   â””â”€â”€ urban/  # Urban sensors
â”‚   â”‚   â””â”€â”€ streams/    # Real-time streams
â”‚   â”‚       â”œâ”€â”€ ingest.py # Stream ingestion
â”‚   â”‚       â””â”€â”€ process.py# Stream processing
â”‚   â”‚
â”‚   â”œâ”€â”€ memories/       # Memory System
â”‚   â”‚   â”œâ”€â”€ store/     # Storage backend
â”‚   â”‚   â”‚   â”œâ”€â”€ vector.py# Vector store
â”‚   â”‚   â”‚   â””â”€â”€ index.py # Indexing system
â”‚   â”‚   â”œâ”€â”€ formation/ # Memory creation
â”‚   â”‚   â”‚   â”œâ”€â”€ create.py# Memory formation
â”‚   â”‚   â”‚   â””â”€â”€ update.py# Memory updates
â”‚   â”‚   â””â”€â”€ query/     # Query system
â”‚   â”‚       â”œâ”€â”€ spatial.py # Spatial queries
â”‚   â”‚       â””â”€â”€ temporal.py# Temporal queries
â”‚   â”‚
â”‚   â”œâ”€â”€ models/        # AI Models
â”‚   â”‚   â”œâ”€â”€ embedding/ # Embedding models
â”‚   â”‚   â”‚   â”œâ”€â”€ text.py  # Text embeddings
â”‚   â”‚   â”‚   â””â”€â”€ vision.py# Vision embeddings
â”‚   â”‚   â”œâ”€â”€ reasoning/# Reasoning models
â”‚   â”‚   â”‚   â”œâ”€â”€ llm.py   # Language models
â”‚   â”‚   â”‚   â””â”€â”€ chain.py # Reasoning chains
â”‚   â”‚   â””â”€â”€ fusion/   # Multi-modal fusion
â”‚   â”‚       â””â”€â”€ combine.py# Modality fusion
â”‚   â”‚
â”‚   â”œâ”€â”€ synthesis/    # Memory Synthesis
â”‚   â”‚   â”œâ”€â”€ fusion/  # Data fusion
â”‚   â”‚   â”‚   â”œâ”€â”€ spatial.py # Spatial fusion
â”‚   â”‚   â”‚   â””â”€â”€ temporal.py# Temporal fusion
â”‚   â”‚   â””â”€â”€ generation/# Synthetic data
â”‚   â”‚       â”œâ”€â”€ augment.py # Data augmentation
â”‚   â”‚       â””â”€â”€ create.py  # Synthetic creation
â”‚   â”‚
â”‚   â””â”€â”€ utils/       # Utilities
â”‚       â”œâ”€â”€ config/  # Configuration
â”‚       â”œâ”€â”€ logging/ # Logging system
â”‚       â””â”€â”€ validation/# Data validation
â”‚
â”œâ”€â”€ tests/           # Test Suite
â”‚   â”œâ”€â”€ unit/       # Unit tests
â”‚   â””â”€â”€ integration/# Integration tests
â”‚
â””â”€â”€ docs/           # Documentation
    â”œâ”€â”€ api/        # API documentation
    â””â”€â”€ guides/     # User guides
```

## ðŸ§© Core Components Explained

### 1. ðŸ¤– Agent System
The agent system is the intelligence layer that orchestrates memory operations:

```python
from memories_dev.agents.specialized import AnalysisAgent

# Create an analysis agent
agent = AnalysisAgent(
    capabilities=["spatial", "temporal", "pattern"],
    memory_access="read_write"
)

# Execute analysis
results = agent.analyze(
    data_sources=["satellite", "sensors"],
    time_range=("2024-01", "2024-03")
)
```

### 2. ðŸ“¡ Data Acquisition
Handles multi-modal data ingestion:

```python
from memories_dev.data_acquisition.satellite import SentinelCollector
from memories_dev.data_acquisition.sensors import ClimateNetwork

# Initialize collectors
satellite = SentinelCollector(
    bands=["B02", "B03", "B04", "B08"]
)

climate = ClimateNetwork(
    metrics=["temperature", "humidity", "air_quality"]
)

# Collect data
data = {
    "satellite": satellite.collect(location=(37.7749, -122.4194)),
    "climate": climate.get_readings(timeframe="1d")
}
```

### 3. ðŸ§  Memory Formation
Memory creation and storage:

```python
from memories_dev.memories.formation import MemoryCreator
from memories_dev.memories.store import VectorStore

# Initialize memory system
store = VectorStore(dimension=1536)
creator = MemoryCreator(store=store)

# Create memory
memory = creator.create(
    content=data,
    metadata={
        "source": "environmental",
        "confidence": 0.95
    }
)
```

## ðŸŽ¯ Features (v1.0.0)

### Core Capabilities
- ðŸ”„ Real-time memory formation and updates
- ðŸ” Advanced spatial and temporal querying
- ðŸ¤ Multi-agent collaboration
- ðŸŽ¯ Context-aware memory retrieval
- ðŸ”— Cross-modal memory linking

### Technical Features
- âš¡ High-performance vector storage
- ðŸ” Secure memory management
- ðŸ“Š Advanced data fusion algorithms
- ðŸŽ¨ Multi-modal data support
- ðŸ”„ Real-time stream processing

## ðŸš€ Roadmap

### Coming in v1.1.0
- Enhanced memory compression
- Improved cross-modal reasoning
- Advanced context understanding

### Future Horizons
- Distributed memory networks
- Quantum-inspired memory storage
- Advanced consciousness simulation

## ðŸ› ï¸ Development

### Prerequisites
- Python 3.8+
- CUDA-compatible GPU (recommended)
- Vector store backend (Redis/Milvus/Pinecone)
- Docker & Docker Compose
- Make (optional, for using Makefile commands)

### Environment Setup

1. **Clone and Setup**
```bash
# Clone the repository
git clone https://github.com/yourusername/memories.dev.git
cd memories.dev

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: .\venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
pip install -r requirements-dev.txt  # Development dependencies
```

2. **Environment Variables**
```bash
# Copy example environment file
cp .env.example .env

# Edit .env with your configurations
nano .env
```

3. **Development Database**
```bash
# Start development services
docker-compose up -d
```

### Development Workflow

#### 1. Code Style
We use `black` for code formatting and `flake8` for linting:
```bash
# Format code
make format

# Run linting
make lint
```

#### 2. Testing
```bash
# Run all tests
make test

# Run specific test file
pytest tests/test_memories/test_store.py

# Run with coverage
make coverage
```

#### 3. Pre-commit Hooks
```bash
# Install pre-commit hooks
pre-commit install

# Run hooks manually
pre-commit run --all-files
```

### ðŸ³ Docker Development

```bash
# Build development image
docker build -t memories-dev -f docker/Dockerfile.dev .

# Run development container
docker run -it --gpus all -v $(pwd):/app memories-dev
```

### ðŸ“Š Monitoring & Debugging

1. **Logging**
```python
# In your code
from memories.utils.logging import get_logger

logger = get_logger(__name__)
logger.info("Memory operation completed")
```

2. **Performance Monitoring**
- Access metrics dashboard: `http://localhost:8080/metrics`
- Prometheus integration available
- Grafana dashboards in `./monitoring`

### ðŸ”„ Common Workflows

#### Adding a New Feature
1. Create feature branch: `git checkout -b feature/your-feature`
2. Implement changes
3. Add tests in `tests/`
4. Update documentation
5. Run test suite: `make test`
6. Create pull request

#### Updating Dependencies
1. Update `requirements.in` or `requirements-dev.in`
2. Compile requirements: `make requirements`
3. Test changes: `make test`
4. Commit changes

## ðŸš€ CI/CD Pipeline

```mermaid
graph LR
    A[Push] --> B[Lint]
    B --> C[Test]
    C --> D[Build]
    D --> E[Deploy]
```

### Automated Checks
- Code formatting (black)
- Type checking (mypy)
- Unit tests (pytest)
- Integration tests
- Security scanning
- Performance benchmarks

### Deployment Environments
- **Dev**: Automatic deployment on main branch
- **Staging**: Manual trigger from release branches
- **Production**: Tagged releases only

## ðŸ“š Documentation

### Building Docs Locally
```bash
# Install documentation dependencies
pip install -r docs/requirements.txt

# Build documentation
cd docs
make html

# Serve documentation
python -m http.server -d _build/html
```

### API Documentation
- [API Reference](docs/api.md)
- [Architecture Guide](docs/architecture.md)
- [Development Guide](docs/development.md)
- [Deployment Guide](docs/deployment.md)
- [Contributing Guide](CONTRIBUTING.md)

### Example Notebooks
Find example notebooks in `notebooks/`:
- ðŸ”° Quick Start
- ðŸ§  Memory Operations
- ðŸ¤– Agent Integration
- ðŸ“Š Data Visualization

## ðŸŽ“ Developer Resources

### Tutorials & Guides
- [Getting Started Guide](docs/getting_started.md)
- [Memory System Architecture](docs/architecture.md)
- [Agent Development Guide](docs/agents.md)
- [Custom Memory Store Integration](docs/custom_stores.md)

### Example Use Cases
1. **Persistent Context in Conversations**
   ```python
   # Example of maintaining context across conversations
   conversation = memory_system.create_conversation()
   conversation.add_memory("User preference: Dark mode")
   conversation.add_memory("Language: Python")
   
   # Later in the conversation
   context = conversation.get_relevant_context("IDE settings")
   ```

2. **Spatial Memory Analysis**
   ```python
   # Analyzing patterns in spatial memories
   spatial_analysis = memory_system.analyze_spatial(
       location=(37.7749, -122.4194),
       radius_km=5,
       time_range=("2024-01-01", "2024-03-15"),
       analysis_type="pattern_recognition"
   )
   ```

### Best Practices
- Always use context managers for memory operations
- Implement proper error handling
- Use batch operations for multiple memories
- Regularly clean up unused memories
- Monitor memory usage and performance

## ðŸ¤ Community & Support

- [Discord Community](https://discord.gg/memoriesdev)
- [GitHub Discussions](https://github.com/Vortx-AI/memories-dev/discussions)
- [Documentation](https://memoriesdev.readthedocs.io/)
- [Blog](https://memoriesdev.medium.com)

## ðŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

### Getting Started
1. Fork the repository
2. Create your feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

### Code Review Process
1. Automated checks must pass
2. Requires one approved review
3. Must maintain test coverage
4. Documentation updates required

## ðŸ“œ Version History

### v1.0.1 (February 15, 2025)
- Implemented query context agent for improved memory retrieval
- Reorganized project structure (memories-dev â†’ memories_dev)
- Enhanced code organization and cleanup
- Improved documentation and README structure

### v1.0.0 (Initial Release, February 14, 2025)
- Initial stable release
- Core memory system implementation
- Basic agent framework
- Multi-modal data support

### v0.9.0 (Beta)
- Feature-complete beta release
- Performance optimizations
- API stabilization





## ðŸ”Œ Integration Examples

### 1. Real-time Environmental Monitoring
```python
from memories_dev import MemorySystem
from memories_dev.data_acquisition import StreamCollector
from memories_dev.synthesis import RealTimeSynthesis

# Setup real-time monitoring
stream = StreamCollector(
    sources=["air_quality", "traffic", "noise"],
    buffer_size="1h",
    sampling_rate="1m"
)

# Process and store real-time data
@stream.on_data
async def process_environmental_data(data):
    # Form memory from real-time data
    memory = await memory_system.store_streaming(
        data,
        retention_policy="24h",
        importance_threshold=0.7
    )
    
    # Trigger real-time synthesis
    insights = await RealTimeSynthesis.analyze(
        memory,
        context_window="6h"
    )
```

### 2. Multi-modal Memory Fusion
```python
from memories_dev.fusion import ModalityFusion
from memories_dev.models import MultiModalEncoder

# Initialize fusion system
fusion = ModalityFusion(
    modalities=["text", "vision", "sensor"],
    fusion_strategy="hierarchical",
    alignment="temporal"
)

# Fuse different memory types
fused_memory = fusion.combine(
    memories={
        "text": text_memory,
        "vision": image_memory,
        "sensor": sensor_data
    },
    weights={
        "text": 0.4,
        "vision": 0.4,
        "sensor": 0.2
    }
)
```

# memories Core Package

## Architecture Overview

```mermaid
graph TB
    A[Client Application] --> B[Memory System]
    B --> C[Hot Memory]
    B --> D[Warm Memory]
    B --> E[Cold Memory]
    B --> F[Glacier Memory]
    
    C --> G[Redis Cache]
    D --> H[Vector Store]
    E --> I[Object Store]
    F --> J[Archive Store]
    
    K[Data Sources] --> L[Data Acquisition]
    L --> M[Memory Formation]
    M --> B
    
    N[Models] --> O[Memory Processing]
    O --> M
```

## Component Structure

```mermaid
classDiagram
    class MemoryStore {
        +create_memories()
        +query_memories()
        +update_memories()
        +delete_memories()
    }
    
    class HotMemory {
        +cache_data()
        +get_cached()
        +invalidate_cache()
    }
    
    class WarmMemory {
        +store_vectors()
        +search_similar()
        +update_index()
    }
    
    class ColdMemory {
        +store_objects()
        +retrieve_objects()
        +compress_data()
    }
    
    class GlacierMemory {
        +archive_data()
        +restore_data()
        +verify_integrity()
    }
    
    MemoryStore --> HotMemory
    MemoryStore --> WarmMemory
    MemoryStore --> ColdMemory
    MemoryStore --> GlacierMemory
```

## Package Structure

```
memories/
â”œâ”€â”€ agents/              # Agent System
â”‚   â”œâ”€â”€ agent_coder.py   # Code generation agent
â”‚   â”œâ”€â”€ agent_context.py # Context understanding
â”‚   â””â”€â”€ agent.py        # Base agent class
â”‚
â”œâ”€â”€ core/               # Core Memory System
â”‚   â”œâ”€â”€ memory.py      # Main memory interface
â”‚   â”œâ”€â”€ hot.py         # Hot memory implementation
â”‚   â”œâ”€â”€ warm.py        # Warm memory implementation
â”‚   â”œâ”€â”€ cold.py        # Cold memory implementation
â”‚   â””â”€â”€ glacier.py     # Glacier memory implementation
â”‚
â”œâ”€â”€ data_acquisition/   # Data Collection
â”‚   â”œâ”€â”€ satellite/     # Satellite data handlers
â”‚   â”œâ”€â”€ sensors/       # Sensor data handlers
â”‚   â””â”€â”€ streams/       # Real-time streams
â”‚
â”œâ”€â”€ models/            # AI Models
â”‚   â”œâ”€â”€ load_model.py  # Model loading utilities
â”‚   â””â”€â”€ config/       # Model configurations
â”‚
â”œâ”€â”€ synthetic/         # Synthetic Data Generation
â”‚   â”œâ”€â”€ generator.py   # Data generation
â”‚   â””â”€â”€ augment.py    # Data augmentation
â”‚
â””â”€â”€ utils/            # Utilities
    â”œâ”€â”€ processors.py # Data processors
    â””â”€â”€ validators.py # Data validators
```

## Memory Tiers

### Hot Memory
- In-memory cache using Redis
- Fastest access times
- Stores frequently accessed data
- Automatic cache invalidation

### Warm Memory
- Vector store for similarity search
- Fast retrieval of related memories
- Efficient indexing and updates
- Supports semantic search

### Cold Memory
- Object storage for raw data
- Compressed storage format
- Batch processing support
- Cost-effective storage

### Glacier Memory
- Long-term archival storage
- High durability guarantee
- Infrequent access pattern
- Data integrity verification

## Data Flow

```mermaid
sequenceDiagram
    participant C as Client
    participant M as MemoryStore
    participant H as HotMemory
    participant W as WarmMemory
    participant D as DataAcquisition
    
    C->>M: Request Memory
    M->>H: Check Cache
    alt Cache Hit
        H-->>M: Return Cached Data
        M-->>C: Return Memory
    else Cache Miss
        M->>W: Search Vector Store
        W-->>M: Return Similar Memories
        M->>D: Acquire New Data
        D-->>M: Return Raw Data
        M->>H: Update Cache
        M-->>C: Return Memory
    end
```

## Usage Examples

### Basic Memory Operations
```python
from memories.core.memory import MemoryStore
from memories.models.load_model import LoadModel

# Initialize
store = MemoryStore()
model = LoadModel(use_gpu=True)

# Create memories
memories = store.create_memories(
    model=model,
    location=(37.7749, -122.4194),
    time_range=("2024-01-01", "2024-02-01")
)

# Query memories
results = store.query_memories(
    query="urban development",
    location_radius_km=10
)
```

### Advanced Features
```python
from memories.synthetic import generate_synthetic
from memories.agents import Agent

# Generate synthetic data
synthetic = generate_synthetic(
    base_location=(37.7749, -122.4194),
    scenario="urban_development",
    time_steps=10
)

# Use agent for analysis
agent = Agent(
    query="Analyze development patterns",
    context_memories=memories,
    synthetic_data=synthetic
)

insights = agent.analyze()
```

## Performance Considerations

1. **Memory Tier Selection**
   - Hot memory: < 1ms access time
   - Warm memory: < 100ms access time
   - Cold memory: < 1s access time
   - Glacier memory: Minutes to hours

2. **Resource Usage**
   - Redis cache: 1-10GB RAM
   - Vector store: 10-100GB disk
   - Object store: 100GB-10TB disk
   - Archive store: 1TB+ disk

3. **Scaling Factors**
   - Number of memories
   - Memory size
   - Query complexity
   - Update frequency

## Contributing

See the main [CONTRIBUTING.md](../CONTRIBUTING.md) for guidelines on contributing to this package.

<p align="center">Built with ðŸ’œ by the memories.dev team</p>

<p align="center">
<a href="https://discord.com/invite/7qAFEekp">Discord</a> â€¢
</p>
